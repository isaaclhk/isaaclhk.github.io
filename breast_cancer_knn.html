<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Isaac Lam">

<title>Breast Cancer Prediction with K-Nearest Neighbours and Visualization with Principal Component Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="breast_cancer_knn_files/libs/clipboard/clipboard.min.js"></script>
<script src="breast_cancer_knn_files/libs/quarto-html/quarto.js"></script>
<script src="breast_cancer_knn_files/libs/quarto-html/popper.min.js"></script>
<script src="breast_cancer_knn_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="breast_cancer_knn_files/libs/quarto-html/anchor.min.js"></script>
<link href="breast_cancer_knn_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="breast_cancer_knn_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="breast_cancer_knn_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="breast_cancer_knn_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="breast_cancer_knn_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a>
  <ul class="collapse">
  <li><a href="#about-the-dataset" id="toc-about-the-dataset" class="nav-link" data-scroll-target="#about-the-dataset">About the Dataset</a></li>
  </ul></li>
  <li><a href="#exploratory-data-analysis" id="toc-exploratory-data-analysis" class="nav-link" data-scroll-target="#exploratory-data-analysis">Exploratory Data Analysis</a></li>
  <li><a href="#principal-component-analysis-pca" id="toc-principal-component-analysis-pca" class="nav-link" data-scroll-target="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a>
  <ul class="collapse">
  <li><a href="#pre-pca-normalization" id="toc-pre-pca-normalization" class="nav-link" data-scroll-target="#pre-pca-normalization">Pre-PCA Normalization</a></li>
  <li><a href="#pca-implementation" id="toc-pca-implementation" class="nav-link" data-scroll-target="#pca-implementation">PCA Implementation</a></li>
  </ul></li>
  <li><a href="#k-nearest-neighbours-knn" id="toc-k-nearest-neighbours-knn" class="nav-link" data-scroll-target="#k-nearest-neighbours-knn">K-Nearest Neighbours (KNN)</a>
  <ul class="collapse">
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a></li>
  <li><a href="#knn-implementation" id="toc-knn-implementation" class="nav-link" data-scroll-target="#knn-implementation">KNN Implementation</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Breast Cancer Prediction with K-Nearest Neighbours and Visualization with Principal Component Analysis</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Isaac Lam </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this project, we will revisit the Wisconsin Breast Cancer dataset. In a <a href="https://github.com/isaaclhk/Projects/blob/main/Python%20projects/breast%20cancer%20prediction.md">previous project</a>, we’ve built a logistic regression model to predict the malignancy of a breast tumor based on its cell nuclei characteristics.</p>
<p>This instance, we will take a second look at this dataset and visualize the data after applying principal component analysis (PCA). In addition, we will build a another prediction model using the K-Nearest Neighbours (KNN) algorithm.</p>
<section id="about-the-dataset" class="level3">
<h3 class="anchored" data-anchor-id="about-the-dataset">About the Dataset</h3>
<p>Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”, Optimization Methods and Software 1, 1992, 23-34].</p>
<p>This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/</p>
<p>Also can be found on the <a href="https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic">UCI Machine Learning Repository</a></p>
<p>Attribute Information:</p>
<p>ID number Diagnosis (M = malignant, B = benign) 3-32) Ten real-valued features are computed for each cell nucleus:</p>
<ol type="a">
<li>radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (“coastline approximation” - 1)</li>
</ol>
<p>The mean, standard error and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.</p>
<p>All feature values are recoded with four significant digits.</p>
<p>Missing attribute values: none</p>
<p>Class distribution: 357 benign, 212 malignant</p>
</section>
</section>
<section id="exploratory-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="exploratory-data-analysis">Exploratory Data Analysis</h2>
<p>We begin by importing relevant libraries and loading the dataset before describing and visualizing the data.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#load libraries and dataset</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Hmisc)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">'C:/Users/isaac/OneDrive/Documents/Projects/datasets/data.csv'</span>) <span class="sc">%&gt;%</span> <span class="fu">rename_all</span>(tolower)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#exploratory analysis</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>id is dropped because it is not germane to the analysis, ‘x’ is also dropped because it consists of only null values.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">select</span>(data, <span class="sc">-</span><span class="fu">c</span>(<span class="st">'id'</span>, <span class="st">'x'</span>))</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>'data.frame':   569 obs. of  31 variables:
 $ diagnosis              : chr  "M" "M" "M" "M" ...
 $ radius_mean            : num  18 20.6 19.7 11.4 20.3 ...
 $ texture_mean           : num  10.4 17.8 21.2 20.4 14.3 ...
 $ perimeter_mean         : num  122.8 132.9 130 77.6 135.1 ...
 $ area_mean              : num  1001 1326 1203 386 1297 ...
 $ smoothness_mean        : num  0.1184 0.0847 0.1096 0.1425 0.1003 ...
 $ compactness_mean       : num  0.2776 0.0786 0.1599 0.2839 0.1328 ...
 $ concavity_mean         : num  0.3001 0.0869 0.1974 0.2414 0.198 ...
 $ concave.points_mean    : num  0.1471 0.0702 0.1279 0.1052 0.1043 ...
 $ symmetry_mean          : num  0.242 0.181 0.207 0.26 0.181 ...
 $ fractal_dimension_mean : num  0.0787 0.0567 0.06 0.0974 0.0588 ...
 $ radius_se              : num  1.095 0.543 0.746 0.496 0.757 ...
 $ texture_se             : num  0.905 0.734 0.787 1.156 0.781 ...
 $ perimeter_se           : num  8.59 3.4 4.58 3.44 5.44 ...
 $ area_se                : num  153.4 74.1 94 27.2 94.4 ...
 $ smoothness_se          : num  0.0064 0.00522 0.00615 0.00911 0.01149 ...
 $ compactness_se         : num  0.049 0.0131 0.0401 0.0746 0.0246 ...
 $ concavity_se           : num  0.0537 0.0186 0.0383 0.0566 0.0569 ...
 $ concave.points_se      : num  0.0159 0.0134 0.0206 0.0187 0.0188 ...
 $ symmetry_se            : num  0.03 0.0139 0.0225 0.0596 0.0176 ...
 $ fractal_dimension_se   : num  0.00619 0.00353 0.00457 0.00921 0.00511 ...
 $ radius_worst           : num  25.4 25 23.6 14.9 22.5 ...
 $ texture_worst          : num  17.3 23.4 25.5 26.5 16.7 ...
 $ perimeter_worst        : num  184.6 158.8 152.5 98.9 152.2 ...
 $ area_worst             : num  2019 1956 1709 568 1575 ...
 $ smoothness_worst       : num  0.162 0.124 0.144 0.21 0.137 ...
 $ compactness_worst      : num  0.666 0.187 0.424 0.866 0.205 ...
 $ concavity_worst        : num  0.712 0.242 0.45 0.687 0.4 ...
 $ concave.points_worst   : num  0.265 0.186 0.243 0.258 0.163 ...
 $ symmetry_worst         : num  0.46 0.275 0.361 0.664 0.236 ...
 $ fractal_dimension_worst: num  0.1189 0.089 0.0876 0.173 0.0768 ...</code></pre>
</div>
</div>
<p>We observe the distribution of malignant and benign tumors in this dataset.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#exploratory data analysis</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">describe</span>(data<span class="sc">$</span>diagnosis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>data$diagnosis 
       n  missing distinct 
     569        0        2 
                      
Value          B     M
Frequency    357   212
Proportion 0.627 0.373</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">table</span>(data<span class="sc">$</span>diagnosis), <span class="at">main =</span> <span class="st">'Diagnoses'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="breast_cancer_knn_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>There are 357 benign tumors and 212 malignant tumors in the dataset. Next, the remaining features are described and visualized in a matrix of histograms. This allows us to inspect the general distribution of each feature and potentially detect outliers.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">describe</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>data <span class="sc">%&gt;%</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">keep</span>(is.numeric) <span class="sc">%&gt;%</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() <span class="sc">%&gt;%</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(value)) <span class="sc">+</span> <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">'free'</span>) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">15</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Summmary of Feature Distributions'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="breast_cancer_knn_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Here we separate the features and outcome variable for analysis later.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#seperate features and outcome variable</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">select</span>(data, <span class="sc">-</span>diagnosis)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">select</span>(data, diagnosis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#check features</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "radius_mean"             "texture_mean"           
 [3] "perimeter_mean"          "area_mean"              
 [5] "smoothness_mean"         "compactness_mean"       
 [7] "concavity_mean"          "concave.points_mean"    
 [9] "symmetry_mean"           "fractal_dimension_mean" 
[11] "radius_se"               "texture_se"             
[13] "perimeter_se"            "area_se"                
[15] "smoothness_se"           "compactness_se"         
[17] "concavity_se"            "concave.points_se"      
[19] "symmetry_se"             "fractal_dimension_se"   
[21] "radius_worst"            "texture_worst"          
[23] "perimeter_worst"         "area_worst"             
[25] "smoothness_worst"        "compactness_worst"      
[27] "concavity_worst"         "concave.points_worst"   
[29] "symmetry_worst"          "fractal_dimension_worst"</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">#check outcome</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "diagnosis"</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Change B and M to 0 and 1</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">unclass</span>(<span class="fu">factor</span>(y<span class="sc">$</span>diagnosis)) <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>y
  0   1 
357 212 </code></pre>
</div>
</div>
</section>
<section id="principal-component-analysis-pca" class="level2">
<h2 class="anchored" data-anchor-id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>
<p>Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional representation while preserving as much variation of the feature set as possible. This enables us to visualize or explore the classification power of a high-dimensional dataset.</p>
<p>The PCA can generally be computed in the following 3 steps:</p>
<p><strong>1. Normalize the data</strong></p>
<p>For this analysis, we will use z-score normalization which transforms each feature to have a mean of 0 and standard deviation of 1. The formula for z-score normalization is shown below:</p>
<p><span class="math display">\[
{\LARGE z = \frac{x-u}{\sigma}}
\]</span> <strong>2. Compute the data covariance matrix</strong></p>
<p><span class="math display">\[
{\LARGE Cov(x,y) = \frac{\sum(x_{i}- \bar{x})*(y_{i}-\bar{y})}{N}}
\]</span></p>
<p><strong>3. Project the normalized data onto the principal subspace spanned by the eigenvectors of the data covariance matrix with the corresponding n largest eigenvalues for a PCA of n components.</strong></p>
<pre><code>This projection can be described as:</code></pre>
<p><span class="math display">\[
{\LARGE \tilde x* = \pi_{u}(x*) = BB^Tx*}
\]</span></p>
<p>Where <span class="math inline">\(x*\)</span> refers to <span class="math inline">\(x\)</span> normalized, <span class="math inline">\(pi_{u}\)</span> refers to the projection of <span class="math inline">\(x*\)</span> onto the principal subspace <span class="math inline">\(u\)</span>, and B is the matrix that contains the eigenvectors that belong to the largest eigenvalues as columns, then <span class="math inline">\(B^Tx*\)</span> are the coordinates of the projection with respect to the basis of the principal subspace.</p>
<p>Further details on the derivation of PCA are covered in <a href="https://www.coursera.org/learn/pca-machine-learning">this course</a> (Deisenroth., n.d.).</p>
<p>Now, we will implement PCA in python code.</p>
<section id="pre-pca-normalization" class="level3">
<h3 class="anchored" data-anchor-id="pre-pca-normalization">Pre-PCA Normalization</h3>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">#normalize data</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> r.x</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> r.y</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>pca_scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>pca_x <span class="op">=</span> pca_scaler.fit_transform(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="pca-implementation" class="level3">
<h3 class="anchored" data-anchor-id="pca-implementation">PCA Implementation</h3>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>pca_transformed <span class="op">=</span> pca.fit_transform(pca_x)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>pca_x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(569, 30)</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>pca_transformed.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(569, 2)</code></pre>
</div>
</div>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>plt.close()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'darkgrid'</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>pca_plot <span class="op">=</span> sns.scatterplot(x <span class="op">=</span> pca_transformed[:, <span class="dv">0</span>], y <span class="op">=</span> pca_transformed[:, <span class="dv">1</span>], hue <span class="op">=</span> y)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>plt.legend(title <span class="op">=</span> <span class="st">'Tumor Classification'</span>, labels <span class="op">=</span> [<span class="st">'Malignant'</span>, <span class="st">'Benign'</span>])</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Dimensional Reduction of Breast Cancer Dataset to 2 Components'</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Principal Component 1'</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Principal Component 2'</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>plt.show(pca_plot)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="breast_cancer_knn_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>PCA projects data points onto the lower-dimensional space spanned by the principal components. By visualizing the projected data, we gain a better understanding of the relationships and patterns within the dataset. For example, in the figure above, we see a distinct separation between malignant and benign tumors. This segregation indicates that the data has high predictive strength. It should be noted, however, that the employed PCA technique captures the maximum variance realizable within two dimensions, thus leaving some unaccounted variance from additional dimensions. Consequently, certain benign tumor data points appear within the cluster of malignant tumor data points and vice versa. But this does not necessarily mean that they will be inaccurately predicted,as a predictive model trained on the complete set of features might yield more precise predictions.</p>
</section>
</section>
<section id="k-nearest-neighbours-knn" class="level2">
<h2 class="anchored" data-anchor-id="k-nearest-neighbours-knn">K-Nearest Neighbours (KNN)</h2>
<p>KNN is a non-parametric supervised machine learning algorithm. The basic idea behind the KNN algorithm is to classify a new data point or predict its value based on its proximity to its neighboring data points in the feature space. In other words, it assumes that data points with similar features tend to belong to the same class or have similar output values.</p>
<p>The default method used by sklearn to calculate distance is the Minkowski Distance, which is a generalization of the euclidean distance in ‘c’ dimensions. The formula for Minkowski distance is:</p>
<p><span class="math display">\[
{\LARGE d(x,y) = (\sum_{i=1}^n \vert xi - yi\vert^c)^\frac{1}{c}}
\]</span></p>
<p>Once the pre-specified ‘k’ number of nearest neighbouring data points are identified, a voting mechanism is used to determine the class label for the new data point. Each neighbor gets to vote, and the majority class among the K neighbors is assigned as the predicted class for the new data point. For example, if K = 5 and K nearest neighbours of a new data point are labelled ‘M’, ‘B’, ‘M’, ‘M’, ‘B’, the KNN algorithm assigns the class with the majority votes, which is ‘M’, to the new data point.</p>
<section id="data-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing">Data Preprocessing</h3>
<p>We begin by separating the dataset into training and testing sets. For this analysis, 70% of the data will be used for training before the model is tested on the remaining 30%. Random state is set to 42 to obtain reproducible results.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(x, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, stratify <span class="op">=</span> y, random_state <span class="op">=</span> <span class="dv">42</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>x_train.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(398, 30)</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>x_test.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(171, 30)</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>398</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(y_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>171</code></pre>
</div>
</div>
<p>Since the algorithm makes predictions by calculating distances between data points, we need to scale the data such that all features are brought to a similar range. This ensures that each feature contributes proportionally to the distance calculation and avoids bias that may arise from features having inherently different values or ranges. Normalizing the data can also mitigate the impact of outliers by bringing the data within a similar range and reducing the influence of extreme values.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>scaler.fit(x_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>StandardScaler()</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>x_train_norm <span class="op">=</span> scaler.transform(x_train)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>x_test_norm <span class="op">=</span> scaler.transform(x_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="knn-implementation" class="level3">
<h3 class="anchored" data-anchor-id="knn-implementation">KNN Implementation</h3>
<p>We perform 10 fold cross validation to determine the optimal number of neighbors for this model.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>neighbors <span class="op">=</span> []</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> []</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform 10 fold cross validation</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">2</span>):</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>  neighbors.append(k)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>  knn <span class="op">=</span> KNeighborsClassifier(n_neighbors <span class="op">=</span> k)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>  scores <span class="op">=</span> cross_val_score(knn, x_train_norm, y_train, cv <span class="op">=</span> <span class="dv">10</span>, scoring <span class="op">=</span> <span class="st">'accuracy'</span>)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>  cv_scores.append(np.mean(scores))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We plot the average accuracy obtained from each set of 10 cross validations for every ‘k’ against the number of neighbors.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co">#plotting cv_scores vs K</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>plt.close()</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x <span class="op">=</span> neighbors, y <span class="op">=</span> cv_scores)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Average Accuracy Scores vs Neighbors'</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="breast_cancer_knn_files/figure-html/unnamed-chunk-14-3.png" class="img-fluid" width="672"></p>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">#calculating optimal number of neighbors</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>optimal_k <span class="op">=</span> neighbors[cv_scores.index(<span class="bu">max</span>(cv_scores))]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Optimal K =  </span><span class="sc">{</span>optimal_k<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimal K =  9</code></pre>
</div>
</div>
<p>Based on the above calculation, the optimal number of neighbors is 9. We shall use this value to fit the final KNN model.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co">#fit model</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors <span class="op">=</span> optimal_k)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>knn.fit(x_train_norm, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>KNeighborsClassifier(n_neighbors=9)</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> knn.predict(x_test_norm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Finally, we evaluate the model by calculating the accuracy score and plotting a confusion matrix.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Our model has an accuracy score of: </span><span class="sc">{</span><span class="bu">round</span>(accuracy_score(y_pred, y_test)<span class="op">*</span><span class="dv">100</span>,<span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Our model has an accuracy score of: 95.32</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co">#visualizing confusion matrix</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>confmat <span class="op">=</span> confusion_matrix(y_pred, y_test)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>plt.close()</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>sns.heatmap(confmat, annot <span class="op">=</span> <span class="va">True</span>, linewidths <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>            xticklabels <span class="op">=</span> [<span class="st">'Benign'</span>, <span class="st">'Malignant'</span>], </span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>            yticklabels <span class="op">=</span> [<span class="st">'Benign'</span>, <span class="st">'Malignant'</span>],</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>            fmt <span class="op">=</span> <span class="st">'g'</span>,</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>            cmap <span class="op">=</span> <span class="st">'Blues'</span>)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'confmat'</span>)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'True Diagnosis'</span>)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted Diagnosis'</span>)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="breast_cancer_knn_files/figure-html/unnamed-chunk-16-5.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li>Deisenroth, M. P. (n.d.) <em>Mathematics for Machine Learning: PCA</em> [MOOC]. Coursera. https://www.coursera.org/learn/pca-machine-learning</li>
<li>Wolberg, W H., Street, W N., Mangasarian, O L. (1995, November) Breast Cancer Winconsin (Diagnostic) Data set, Retrieved from https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data.</li>
</ol>
<!-- -->

</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb44" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Breast Cancer Prediction with K-Nearest Neighbours and Visualization with Principal Component Analysis"</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Contents</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-location: right</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: show</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Isaac Lam</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="fu">## Background</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>In this project, we will revisit the Wisconsin Breast Cancer dataset. </span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>In a <span class="co">[</span><span class="ot">previous project</span><span class="co">](https://github.com/isaaclhk/Projects/blob/main/Python%20projects/breast%20cancer%20prediction.md)</span>, we've built a logistic regression model to predict the malignancy of a breast tumor based on its cell nuclei characteristics. </span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>This instance, we will take a second look at this dataset and visualize the data after applying principal component analysis (PCA). In addition, we will build a another prediction model using the K-Nearest Neighbours (KNN) algorithm.</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="fu">### About the Dataset</span></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: <span class="sc">\[</span>K. P. Bennett and O. L. Mangasarian: "Robust Linear Programming Discrimination of Two Linearly Inseparable Sets", Optimization Methods and Software 1, 1992, 23-34<span class="sc">\]</span>.</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>Also can be found on the <span class="co">[</span><span class="ot">UCI Machine Learning Repository</span><span class="co">](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>Attribute Information:</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>ID number Diagnosis (M = malignant, B = benign) 3-32) Ten real-valued features are computed for each cell nucleus:</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>a)  radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter\^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension ("coastline approximation" - 1)</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>The mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a>All feature values are recoded with four significant digits.</span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>Missing attribute values: none</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a>Class distribution: 357 benign, 212 malignant</span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exploratory Data Analysis</span></span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a>We begin by importing relevant libraries and loading the dataset before describing and visualizing the data.</span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a><span class="in">```{R}</span></span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a><span class="co">#load libraries and dataset</span></span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Hmisc)</span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">'C:/Users/isaac/OneDrive/Documents/Projects/datasets/data.csv'</span>) <span class="sc">%&gt;%</span> <span class="fu">rename_all</span>(tolower)</span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a><span class="co">#exploratory analysis</span></span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data)</span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(data)</span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a>id is dropped because it is not germane to the analysis, 'x' is also dropped because it consists of only null values.</span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-68"><a href="#cb44-68" aria-hidden="true" tabindex="-1"></a><span class="in">```{R}</span></span>
<span id="cb44-69"><a href="#cb44-69" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">select</span>(data, <span class="sc">-</span><span class="fu">c</span>(<span class="st">'id'</span>, <span class="st">'x'</span>))</span>
<span id="cb44-70"><a href="#cb44-70" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(data)</span>
<span id="cb44-71"><a href="#cb44-71" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-72"><a href="#cb44-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-73"><a href="#cb44-73" aria-hidden="true" tabindex="-1"></a>We observe the distribution of malignant and benign tumors in this dataset.</span>
<span id="cb44-74"><a href="#cb44-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-77"><a href="#cb44-77" aria-hidden="true" tabindex="-1"></a><span class="in">```{R}</span></span>
<span id="cb44-78"><a href="#cb44-78" aria-hidden="true" tabindex="-1"></a><span class="co">#exploratory data analysis</span></span>
<span id="cb44-79"><a href="#cb44-79" aria-hidden="true" tabindex="-1"></a><span class="fu">describe</span>(data<span class="sc">$</span>diagnosis)</span>
<span id="cb44-80"><a href="#cb44-80" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">table</span>(data<span class="sc">$</span>diagnosis), <span class="at">main =</span> <span class="st">'Diagnoses'</span>)</span>
<span id="cb44-81"><a href="#cb44-81" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-82"><a href="#cb44-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-83"><a href="#cb44-83" aria-hidden="true" tabindex="-1"></a>There are 357 benign tumors and 212 malignant tumors in the dataset. Next, the remaining features are described and visualized in a matrix of histograms. This allows us to inspect the general distribution of each feature and potentially detect outliers.</span>
<span id="cb44-84"><a href="#cb44-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-87"><a href="#cb44-87" aria-hidden="true" tabindex="-1"></a><span class="in">```{R}</span></span>
<span id="cb44-88"><a href="#cb44-88" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb44-89"><a href="#cb44-89" aria-hidden="true" tabindex="-1"></a><span class="fu">describe</span>(data)</span>
<span id="cb44-90"><a href="#cb44-90" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-93"><a href="#cb44-93" aria-hidden="true" tabindex="-1"></a><span class="in">```{R}</span></span>
<span id="cb44-94"><a href="#cb44-94" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb44-95"><a href="#cb44-95" aria-hidden="true" tabindex="-1"></a>data <span class="sc">%&gt;%</span></span>
<span id="cb44-96"><a href="#cb44-96" aria-hidden="true" tabindex="-1"></a>  <span class="fu">keep</span>(is.numeric) <span class="sc">%&gt;%</span></span>
<span id="cb44-97"><a href="#cb44-97" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() <span class="sc">%&gt;%</span></span>
<span id="cb44-98"><a href="#cb44-98" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(value)) <span class="sc">+</span> <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">'free'</span>) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">15</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Summmary of Feature Distributions'</span>)</span>
<span id="cb44-99"><a href="#cb44-99" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-100"><a href="#cb44-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-101"><a href="#cb44-101" aria-hidden="true" tabindex="-1"></a>Here we separate the features and outcome variable for analysis later.</span>
<span id="cb44-102"><a href="#cb44-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-105"><a href="#cb44-105" aria-hidden="true" tabindex="-1"></a><span class="in">```{R}</span></span>
<span id="cb44-106"><a href="#cb44-106" aria-hidden="true" tabindex="-1"></a><span class="co">#seperate features and outcome variable</span></span>
<span id="cb44-107"><a href="#cb44-107" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">select</span>(data, <span class="sc">-</span>diagnosis)</span>
<span id="cb44-108"><a href="#cb44-108" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">select</span>(data, diagnosis)</span>
<span id="cb44-109"><a href="#cb44-109" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-112"><a href="#cb44-112" aria-hidden="true" tabindex="-1"></a><span class="in">```{R}</span></span>
<span id="cb44-113"><a href="#cb44-113" aria-hidden="true" tabindex="-1"></a><span class="co">#check features</span></span>
<span id="cb44-114"><a href="#cb44-114" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(x)</span>
<span id="cb44-115"><a href="#cb44-115" aria-hidden="true" tabindex="-1"></a><span class="co">#check outcome</span></span>
<span id="cb44-116"><a href="#cb44-116" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(y)</span>
<span id="cb44-117"><a href="#cb44-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-118"><a href="#cb44-118" aria-hidden="true" tabindex="-1"></a><span class="co">#Change B and M to 0 and 1</span></span>
<span id="cb44-119"><a href="#cb44-119" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">unclass</span>(<span class="fu">factor</span>(y<span class="sc">$</span>diagnosis)) <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb44-120"><a href="#cb44-120" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(y)</span>
<span id="cb44-121"><a href="#cb44-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-122"><a href="#cb44-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-123"><a href="#cb44-123" aria-hidden="true" tabindex="-1"></a><span class="fu">## Principal Component Analysis (PCA)</span></span>
<span id="cb44-124"><a href="#cb44-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-125"><a href="#cb44-125" aria-hidden="true" tabindex="-1"></a>Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional representation while preserving as much variation of the feature set as possible. This enables us to visualize or explore the classification power of a high-dimensional dataset.</span>
<span id="cb44-126"><a href="#cb44-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-127"><a href="#cb44-127" aria-hidden="true" tabindex="-1"></a>The PCA can generally be computed in the following 3 steps:</span>
<span id="cb44-128"><a href="#cb44-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-129"><a href="#cb44-129" aria-hidden="true" tabindex="-1"></a>**1. Normalize the data**</span>
<span id="cb44-130"><a href="#cb44-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-131"><a href="#cb44-131" aria-hidden="true" tabindex="-1"></a>For this analysis, we will use z-score normalization which transforms each feature to have a mean of 0 and standard deviation of 1. The formula for z-score normalization is shown below:</span>
<span id="cb44-132"><a href="#cb44-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-133"><a href="#cb44-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-134"><a href="#cb44-134" aria-hidden="true" tabindex="-1"></a>{\LARGE z = \frac{x-u}{\sigma}}</span>
<span id="cb44-135"><a href="#cb44-135" aria-hidden="true" tabindex="-1"></a>$$ **2. Compute the data covariance matrix**</span>
<span id="cb44-136"><a href="#cb44-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-137"><a href="#cb44-137" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-138"><a href="#cb44-138" aria-hidden="true" tabindex="-1"></a>{\LARGE Cov(x,y) = \frac{\sum(x_{i}- \bar{x})*(y_{i}-\bar{y})}{N}}</span>
<span id="cb44-139"><a href="#cb44-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-140"><a href="#cb44-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-141"><a href="#cb44-141" aria-hidden="true" tabindex="-1"></a>**3. Project the normalized data onto the principal subspace spanned by the eigenvectors of the data covariance matrix with the corresponding n largest eigenvalues for a PCA of n components.**</span>
<span id="cb44-142"><a href="#cb44-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-143"><a href="#cb44-143" aria-hidden="true" tabindex="-1"></a><span class="in">    This projection can be described as:</span></span>
<span id="cb44-144"><a href="#cb44-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-145"><a href="#cb44-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-146"><a href="#cb44-146" aria-hidden="true" tabindex="-1"></a>{\LARGE \tilde x* = \pi_{u}(x*) = BB^Tx*}</span>
<span id="cb44-147"><a href="#cb44-147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-148"><a href="#cb44-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-149"><a href="#cb44-149" aria-hidden="true" tabindex="-1"></a>Where $x*$ refers to $x$ normalized, $pi_{u}$ refers to the projection of $x*$ onto the principal subspace $u$, and B is the matrix that contains the eigenvectors that belong to the largest eigenvalues as columns, then $B^Tx*$ are the coordinates of the projection with respect to the basis of the principal subspace.</span>
<span id="cb44-150"><a href="#cb44-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-151"><a href="#cb44-151" aria-hidden="true" tabindex="-1"></a>Further details on the derivation of PCA are covered in <span class="co">[</span><span class="ot">this course</span><span class="co">](https://www.coursera.org/learn/pca-machine-learning)</span> (Deisenroth., n.d.).</span>
<span id="cb44-152"><a href="#cb44-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-153"><a href="#cb44-153" aria-hidden="true" tabindex="-1"></a>Now, we will implement PCA in python code.</span>
<span id="cb44-154"><a href="#cb44-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-155"><a href="#cb44-155" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pre-PCA Normalization</span></span>
<span id="cb44-156"><a href="#cb44-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-159"><a href="#cb44-159" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb44-160"><a href="#cb44-160" aria-hidden="true" tabindex="-1"></a><span class="co">#normalize data</span></span>
<span id="cb44-161"><a href="#cb44-161" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> r.x</span>
<span id="cb44-162"><a href="#cb44-162" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> r.y</span>
<span id="cb44-163"><a href="#cb44-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-164"><a href="#cb44-164" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb44-165"><a href="#cb44-165" aria-hidden="true" tabindex="-1"></a>pca_scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb44-166"><a href="#cb44-166" aria-hidden="true" tabindex="-1"></a>pca_x <span class="op">=</span> pca_scaler.fit_transform(x)</span>
<span id="cb44-167"><a href="#cb44-167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-168"><a href="#cb44-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-169"><a href="#cb44-169" aria-hidden="true" tabindex="-1"></a><span class="fu">### PCA Implementation</span></span>
<span id="cb44-172"><a href="#cb44-172" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb44-173"><a href="#cb44-173" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb44-174"><a href="#cb44-174" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb44-175"><a href="#cb44-175" aria-hidden="true" tabindex="-1"></a>pca_transformed <span class="op">=</span> pca.fit_transform(pca_x)</span>
<span id="cb44-176"><a href="#cb44-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-177"><a href="#cb44-177" aria-hidden="true" tabindex="-1"></a>pca_x.shape</span>
<span id="cb44-178"><a href="#cb44-178" aria-hidden="true" tabindex="-1"></a>pca_transformed.shape</span>
<span id="cb44-179"><a href="#cb44-179" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-180"><a href="#cb44-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-183"><a href="#cb44-183" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb44-184"><a href="#cb44-184" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb44-185"><a href="#cb44-185" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb44-186"><a href="#cb44-186" aria-hidden="true" tabindex="-1"></a>plt.close()</span>
<span id="cb44-187"><a href="#cb44-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-188"><a href="#cb44-188" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'darkgrid'</span>)</span>
<span id="cb44-189"><a href="#cb44-189" aria-hidden="true" tabindex="-1"></a>pca_plot <span class="op">=</span> sns.scatterplot(x <span class="op">=</span> pca_transformed[:, <span class="dv">0</span>], y <span class="op">=</span> pca_transformed[:, <span class="dv">1</span>], hue <span class="op">=</span> y)</span>
<span id="cb44-190"><a href="#cb44-190" aria-hidden="true" tabindex="-1"></a>plt.legend(title <span class="op">=</span> <span class="st">'Tumor Classification'</span>, labels <span class="op">=</span> [<span class="st">'Malignant'</span>, <span class="st">'Benign'</span>])</span>
<span id="cb44-191"><a href="#cb44-191" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Dimensional Reduction of Breast Cancer Dataset to 2 Components'</span>)</span>
<span id="cb44-192"><a href="#cb44-192" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Principal Component 1'</span>)</span>
<span id="cb44-193"><a href="#cb44-193" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Principal Component 2'</span>)</span>
<span id="cb44-194"><a href="#cb44-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-195"><a href="#cb44-195" aria-hidden="true" tabindex="-1"></a>plt.show(pca_plot)</span>
<span id="cb44-196"><a href="#cb44-196" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-197"><a href="#cb44-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-198"><a href="#cb44-198" aria-hidden="true" tabindex="-1"></a>PCA projects data points onto the lower-dimensional space spanned by the principal components. By visualizing the projected data, we gain a better understanding of the relationships and patterns within the dataset. For example, in the figure above, we see a distinct separation between malignant and benign tumors. This segregation indicates that the data has high predictive strength. It should be noted, however, that the employed PCA technique captures the maximum variance realizable within two dimensions, thus leaving some unaccounted variance from additional dimensions. Consequently, certain benign tumor data points appear within the cluster of malignant tumor data points and vice versa. But this does not necessarily mean that they will be inaccurately predicted,as a predictive model trained on the complete set of features might yield more precise predictions.</span>
<span id="cb44-199"><a href="#cb44-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-200"><a href="#cb44-200" aria-hidden="true" tabindex="-1"></a><span class="fu">## K-Nearest Neighbours (KNN)</span></span>
<span id="cb44-201"><a href="#cb44-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-202"><a href="#cb44-202" aria-hidden="true" tabindex="-1"></a>KNN is a non-parametric supervised machine learning algorithm. The basic idea behind the KNN algorithm is to classify a new data point or predict its value based on its proximity to its neighboring data points in the feature space. In other words, it assumes that data points with similar features tend to belong to the same class or have similar output values.</span>
<span id="cb44-203"><a href="#cb44-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-204"><a href="#cb44-204" aria-hidden="true" tabindex="-1"></a>The default method used by sklearn to calculate distance is the Minkowski Distance, which is a generalization of the euclidean distance in 'c' dimensions. The formula for Minkowski distance is:</span>
<span id="cb44-205"><a href="#cb44-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-206"><a href="#cb44-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-207"><a href="#cb44-207" aria-hidden="true" tabindex="-1"></a>{\LARGE d(x,y) = (\sum_{i=1}^n \vert xi - yi\vert^c)^\frac{1}{c}}</span>
<span id="cb44-208"><a href="#cb44-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-209"><a href="#cb44-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-210"><a href="#cb44-210" aria-hidden="true" tabindex="-1"></a>Once the pre-specified 'k' number of nearest neighbouring data points are identified, a voting mechanism is used to determine the class label for the new data point. Each neighbor gets to vote, and the majority class among the K neighbors is assigned as the predicted class for the new data point. For example, if K = 5 and K nearest neighbours of a new data point are labelled 'M', 'B', 'M', 'M', 'B', the KNN algorithm assigns the class with the majority votes, which is 'M', to the new data point.</span>
<span id="cb44-211"><a href="#cb44-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-212"><a href="#cb44-212" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data Preprocessing</span></span>
<span id="cb44-213"><a href="#cb44-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-214"><a href="#cb44-214" aria-hidden="true" tabindex="-1"></a>We begin by separating the dataset into training and testing sets. For this analysis, 70% of the data will be used for training before the model is tested on the remaining 30%. Random state is set to 42 to obtain reproducible results.</span>
<span id="cb44-215"><a href="#cb44-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-218"><a href="#cb44-218" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb44-219"><a href="#cb44-219" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb44-220"><a href="#cb44-220" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(x, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, stratify <span class="op">=</span> y, random_state <span class="op">=</span> <span class="dv">42</span>)</span>
<span id="cb44-221"><a href="#cb44-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-222"><a href="#cb44-222" aria-hidden="true" tabindex="-1"></a>x_train.shape</span>
<span id="cb44-223"><a href="#cb44-223" aria-hidden="true" tabindex="-1"></a>x_test.shape</span>
<span id="cb44-224"><a href="#cb44-224" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(y_train)</span>
<span id="cb44-225"><a href="#cb44-225" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(y_test)</span>
<span id="cb44-226"><a href="#cb44-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-227"><a href="#cb44-227" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-228"><a href="#cb44-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-229"><a href="#cb44-229" aria-hidden="true" tabindex="-1"></a>Since the algorithm makes predictions by calculating distances between data points, we need to scale the data such that all features are brought to a similar range. This ensures that each feature contributes proportionally to the distance calculation and avoids bias that may arise from features having inherently different values or ranges. Normalizing the data can also mitigate the impact of outliers by bringing the data within a similar range and reducing the influence of extreme values.</span>
<span id="cb44-230"><a href="#cb44-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-233"><a href="#cb44-233" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb44-234"><a href="#cb44-234" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb44-235"><a href="#cb44-235" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb44-236"><a href="#cb44-236" aria-hidden="true" tabindex="-1"></a>scaler.fit(x_train)</span>
<span id="cb44-237"><a href="#cb44-237" aria-hidden="true" tabindex="-1"></a>x_train_norm <span class="op">=</span> scaler.transform(x_train)</span>
<span id="cb44-238"><a href="#cb44-238" aria-hidden="true" tabindex="-1"></a>x_test_norm <span class="op">=</span> scaler.transform(x_test)</span>
<span id="cb44-239"><a href="#cb44-239" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-240"><a href="#cb44-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-241"><a href="#cb44-241" aria-hidden="true" tabindex="-1"></a><span class="fu">### KNN Implementation</span></span>
<span id="cb44-242"><a href="#cb44-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-243"><a href="#cb44-243" aria-hidden="true" tabindex="-1"></a>We perform 10 fold cross validation to determine the optimal number of neighbors for this model.</span>
<span id="cb44-244"><a href="#cb44-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-247"><a href="#cb44-247" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb44-248"><a href="#cb44-248" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb44-249"><a href="#cb44-249" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb44-250"><a href="#cb44-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-251"><a href="#cb44-251" aria-hidden="true" tabindex="-1"></a>neighbors <span class="op">=</span> []</span>
<span id="cb44-252"><a href="#cb44-252" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> []</span>
<span id="cb44-253"><a href="#cb44-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-254"><a href="#cb44-254" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb44-255"><a href="#cb44-255" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform 10 fold cross validation</span></span>
<span id="cb44-256"><a href="#cb44-256" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">2</span>):</span>
<span id="cb44-257"><a href="#cb44-257" aria-hidden="true" tabindex="-1"></a>  neighbors.append(k)</span>
<span id="cb44-258"><a href="#cb44-258" aria-hidden="true" tabindex="-1"></a>  knn <span class="op">=</span> KNeighborsClassifier(n_neighbors <span class="op">=</span> k)</span>
<span id="cb44-259"><a href="#cb44-259" aria-hidden="true" tabindex="-1"></a>  scores <span class="op">=</span> cross_val_score(knn, x_train_norm, y_train, cv <span class="op">=</span> <span class="dv">10</span>, scoring <span class="op">=</span> <span class="st">'accuracy'</span>)</span>
<span id="cb44-260"><a href="#cb44-260" aria-hidden="true" tabindex="-1"></a>  cv_scores.append(np.mean(scores))</span>
<span id="cb44-261"><a href="#cb44-261" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-262"><a href="#cb44-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-263"><a href="#cb44-263" aria-hidden="true" tabindex="-1"></a>We plot the average accuracy obtained from each set of 10 cross validations for every 'k' against the number of neighbors.</span>
<span id="cb44-264"><a href="#cb44-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-267"><a href="#cb44-267" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb44-268"><a href="#cb44-268" aria-hidden="true" tabindex="-1"></a><span class="co">#plotting cv_scores vs K</span></span>
<span id="cb44-269"><a href="#cb44-269" aria-hidden="true" tabindex="-1"></a>plt.close()</span>
<span id="cb44-270"><a href="#cb44-270" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x <span class="op">=</span> neighbors, y <span class="op">=</span> cv_scores)</span>
<span id="cb44-271"><a href="#cb44-271" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Average Accuracy Scores vs Neighbors'</span>)</span>
<span id="cb44-272"><a href="#cb44-272" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb44-273"><a href="#cb44-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-274"><a href="#cb44-274" aria-hidden="true" tabindex="-1"></a><span class="co">#calculating optimal number of neighbors</span></span>
<span id="cb44-275"><a href="#cb44-275" aria-hidden="true" tabindex="-1"></a>optimal_k <span class="op">=</span> neighbors[cv_scores.index(<span class="bu">max</span>(cv_scores))]</span>
<span id="cb44-276"><a href="#cb44-276" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Optimal K =  </span><span class="sc">{</span>optimal_k<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb44-277"><a href="#cb44-277" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-278"><a href="#cb44-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-279"><a href="#cb44-279" aria-hidden="true" tabindex="-1"></a>Based on the above calculation, the optimal number of neighbors is 9. We shall use this value to fit the final KNN model.</span>
<span id="cb44-280"><a href="#cb44-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-283"><a href="#cb44-283" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb44-284"><a href="#cb44-284" aria-hidden="true" tabindex="-1"></a><span class="co">#fit model</span></span>
<span id="cb44-285"><a href="#cb44-285" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors <span class="op">=</span> optimal_k)</span>
<span id="cb44-286"><a href="#cb44-286" aria-hidden="true" tabindex="-1"></a>knn.fit(x_train_norm, y_train)</span>
<span id="cb44-287"><a href="#cb44-287" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> knn.predict(x_test_norm)</span>
<span id="cb44-288"><a href="#cb44-288" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-289"><a href="#cb44-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-290"><a href="#cb44-290" aria-hidden="true" tabindex="-1"></a>Finally, we evaluate the model by calculating the accuracy score and plotting a confusion matrix.</span>
<span id="cb44-293"><a href="#cb44-293" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb44-294"><a href="#cb44-294" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb44-295"><a href="#cb44-295" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb44-296"><a href="#cb44-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-297"><a href="#cb44-297" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Our model has an accuracy score of: </span><span class="sc">{</span><span class="bu">round</span>(accuracy_score(y_pred, y_test)<span class="op">*</span><span class="dv">100</span>,<span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb44-298"><a href="#cb44-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-299"><a href="#cb44-299" aria-hidden="true" tabindex="-1"></a><span class="co">#visualizing confusion matrix</span></span>
<span id="cb44-300"><a href="#cb44-300" aria-hidden="true" tabindex="-1"></a>confmat <span class="op">=</span> confusion_matrix(y_pred, y_test)</span>
<span id="cb44-301"><a href="#cb44-301" aria-hidden="true" tabindex="-1"></a>plt.close()</span>
<span id="cb44-302"><a href="#cb44-302" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb44-303"><a href="#cb44-303" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb44-304"><a href="#cb44-304" aria-hidden="true" tabindex="-1"></a>sns.heatmap(confmat, annot <span class="op">=</span> <span class="va">True</span>, linewidths <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb44-305"><a href="#cb44-305" aria-hidden="true" tabindex="-1"></a>            xticklabels <span class="op">=</span> [<span class="st">'Benign'</span>, <span class="st">'Malignant'</span>], </span>
<span id="cb44-306"><a href="#cb44-306" aria-hidden="true" tabindex="-1"></a>            yticklabels <span class="op">=</span> [<span class="st">'Benign'</span>, <span class="st">'Malignant'</span>],</span>
<span id="cb44-307"><a href="#cb44-307" aria-hidden="true" tabindex="-1"></a>            fmt <span class="op">=</span> <span class="st">'g'</span>,</span>
<span id="cb44-308"><a href="#cb44-308" aria-hidden="true" tabindex="-1"></a>            cmap <span class="op">=</span> <span class="st">'Blues'</span>)</span>
<span id="cb44-309"><a href="#cb44-309" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'confmat'</span>)</span>
<span id="cb44-310"><a href="#cb44-310" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'True Diagnosis'</span>)</span>
<span id="cb44-311"><a href="#cb44-311" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted Diagnosis'</span>)</span>
<span id="cb44-312"><a href="#cb44-312" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb44-313"><a href="#cb44-313" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-314"><a href="#cb44-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-315"><a href="#cb44-315" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb44-316"><a href="#cb44-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-317"><a href="#cb44-317" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Deisenroth, M. P. (n.d.) *Mathematics for Machine Learning: PCA* <span class="sc">\[</span>MOOC<span class="sc">\]</span>. Coursera. https://www.coursera.org/learn/pca-machine-learning</span>
<span id="cb44-318"><a href="#cb44-318" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Wolberg, W H., Street, W N., Mangasarian, O L. (1995, November) Breast Cancer Winconsin (Diagnostic) Data set, Retrieved from https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>